{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cricket Match Data Processor\n",
    "\n",
    "This notebook processes multiple cricket match JSON files to extract only the cricket_match_extended_ball data and save it as CSV files named in the format `match_id__team_id.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any, Union, Optional\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d: Dict, parent_key: str = '', sep: str = '_') -> Dict:\n",
    "    \"\"\"\n",
    "    Flatten a nested dictionary structure.\n",
    "    \n",
    "    Args:\n",
    "        d: The dictionary to flatten\n",
    "        parent_key: The parent key for the current dictionary\n",
    "        sep: Separator between keys\n",
    "        \n",
    "    Returns:\n",
    "        Flattened dictionary\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        \n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "            # For lists, we'll stringify them unless they're a list of dicts\n",
    "            if v and isinstance(v[0], dict):\n",
    "                # If it's a list of dicts, we'll just use the first item\n",
    "                # This is a simplification - you may want to handle this differently\n",
    "                items.extend(flatten_dict(v[0], new_key, sep=sep).items())\n",
    "            else:\n",
    "                items.append((new_key, str(v)))\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_extract_cricket_match_extended_ball(data_array: List[Dict]) -> tuple:\n",
    "    \"\"\"\n",
    "    Process commentary_with_extended_summary and extract only cricket_match_extended_ball data.\n",
    "    \n",
    "    Args:\n",
    "        data_array: List of dictionaries with {type, data} structure\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (DataFrame containing only cricket_match_extended_ball data, match_id, team_id) or (None, None, None) if not found\n",
    "    \"\"\"\n",
    "    # Extract only items of type 'cricket_match_extended_ball'\n",
    "    ball_data = []\n",
    "    match_id = None\n",
    "    team_id = None\n",
    "    \n",
    "    for item in data_array:\n",
    "        if item.get('type') == 'cricket_match_extended_ball':\n",
    "            # Flatten the data part of the structure\n",
    "            if 'data' in item and isinstance(item['data'], dict):\n",
    "                flattened_data = flatten_dict(item['data'])\n",
    "                ball_data.append(flattened_data)\n",
    "                \n",
    "                # Extract match_id and team_id from the first item\n",
    "                if match_id is None and 'match_id' in flattened_data:\n",
    "                    match_id = str(flattened_data['match_id'])\n",
    "                if team_id is None and 'batting_team_id' in flattened_data:\n",
    "                    team_id = str(flattened_data['batting_team_id'])\n",
    "    \n",
    "    if not ball_data:\n",
    "        return None, None, None\n",
    "        \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(ball_data)\n",
    "    return df, match_id, team_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_csv(json_data: Union[str, Dict], output_dir: str = '.') -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Convert JSON data to CSV format, extracting only cricket_match_extended_ball data\n",
    "    and naming the file as match_id__team_id.csv\n",
    "    \n",
    "    Args:\n",
    "        json_data: JSON data either as a string or parsed dictionary\n",
    "        output_dir: Directory to save the output CSV file\n",
    "    \n",
    "    Returns:\n",
    "        Path to the generated CSV file or None if error\n",
    "    \"\"\"\n",
    "    # Parse JSON if it's a string\n",
    "    if isinstance(json_data, str):\n",
    "        try:\n",
    "            data = json.loads(json_data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        data = json_data\n",
    "    \n",
    "    # Check if the data has the expected structure\n",
    "    if not data.get('status') or not data.get('data'):\n",
    "        print(\"Invalid JSON structure. Expected format: { status: boolean, data: {...} }\")\n",
    "        return None\n",
    "    \n",
    "    # Process only commentary_with_extended_summary to extract cricket_match_extended_ball\n",
    "    if 'commentary_with_extended_summary' in data['data'] and data['data']['commentary_with_extended_summary']:\n",
    "        df, match_id, team_id = process_and_extract_cricket_match_extended_ball(\n",
    "            data['data']['commentary_with_extended_summary']\n",
    "        )\n",
    "        \n",
    "        if df is not None:\n",
    "            # Use match_id and team_id for the filename\n",
    "            if match_id and team_id:\n",
    "                output_file = os.path.join(output_dir, f\"{match_id}__{team_id}.csv\")\n",
    "            else:\n",
    "                output_file = os.path.join(output_dir, \"match_unknown__team_unknown.csv\")\n",
    "            \n",
    "            # Write to CSV\n",
    "            try:\n",
    "                # Create output directory if it doesn't exist\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                \n",
    "                df.to_csv(output_file, index=False)\n",
    "                return output_file\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing CSV file {output_file}: {e}\")\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Multiple Files\n",
    "\n",
    "### Set Input and Output Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your input and output directories here\n",
    "input_dir = \"/Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON\"  # Replace with your input directory path\n",
    "output_dir = \"/Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/\"  # Replace with your output directory path\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all JSON files in the input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 JSON files to process\n"
     ]
    }
   ],
   "source": [
    "# Get list of all JSON files in the input directory\n",
    "json_files = glob.glob(os.path.join(input_dir, \"*.json\"))\n",
    "print(f\"Found {len(json_files)} JSON files to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f62f51feec479e96086fcb5eff9bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully processed 0 files\n",
      "Failed to process 22 files\n"
     ]
    }
   ],
   "source": [
    "# Process each JSON file\n",
    "processed_files = []\n",
    "failed_files = []\n",
    "\n",
    "for json_file in tqdm(json_files, desc=\"Processing files\"):\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(json_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        # Convert to CSV\n",
    "        output_file = convert_json_to_csv(json_data, output_dir)\n",
    "        \n",
    "        if output_file:\n",
    "            processed_files.append((json_file, output_file))\n",
    "        else:\n",
    "            failed_files.append((json_file, \"No cricket_match_extended_ball data found\"))\n",
    "    except Exception as e:\n",
    "        failed_files.append((json_file, str(e)))\n",
    "        \n",
    "print(f\"\\nSuccessfully processed {len(processed_files)} files\")\n",
    "print(f\"Failed to process {len(failed_files)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View processing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed files:\n"
     ]
    }
   ],
   "source": [
    "# Show successful files\n",
    "print(\"Successfully processed files:\")\n",
    "for json_file, csv_file in processed_files:\n",
    "    print(f\"  {os.path.basename(json_file)} -> {os.path.basename(csv_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failed files:\n",
      "  20.json: No cricket_match_extended_ball data found\n",
      "  16.json: No cricket_match_extended_ball data found\n",
      "  6.json: No cricket_match_extended_ball data found\n",
      "  7.json: No cricket_match_extended_ball data found\n",
      "  17.json: No cricket_match_extended_ball data found\n",
      "  21.json: No cricket_match_extended_ball data found\n",
      "  10.json: No cricket_match_extended_ball data found\n",
      "  1.json: No cricket_match_extended_ball data found\n",
      "  11.json: No cricket_match_extended_ball data found\n",
      "  2.json: No cricket_match_extended_ball data found\n",
      "  12.json: No cricket_match_extended_ball data found\n",
      "  13.json: No cricket_match_extended_ball data found\n",
      "  3.json: No cricket_match_extended_ball data found\n",
      "  8.json: No cricket_match_extended_ball data found\n",
      "  22.json: No cricket_match_extended_ball data found\n",
      "  18.json: No cricket_match_extended_ball data found\n",
      "  4.json: No cricket_match_extended_ball data found\n",
      "  14.json: No cricket_match_extended_ball data found\n",
      "  15.json: No cricket_match_extended_ball data found\n",
      "  5.json: No cricket_match_extended_ball data found\n",
      "  19.json: No cricket_match_extended_ball data found\n",
      "  9.json: No cricket_match_extended_ball data found\n"
     ]
    }
   ],
   "source": [
    "# Show failed files\n",
    "if failed_files:\n",
    "    print(\"\\nFailed files:\")\n",
    "    for json_file, error in failed_files:\n",
    "        print(f\"  {os.path.basename(json_file)}: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process a Specific Directory\n",
    "\n",
    "Use this cell if you want to process files in a specific directory other than the one configured above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(input_directory, output_directory):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    # Get list of all JSON files in the input directory\n",
    "    json_files = glob.glob(os.path.join(input_directory, \"*.json\"))\n",
    "    print(f\"Found {len(json_files)} JSON files to process\")\n",
    "    \n",
    "    # Process each JSON file\n",
    "    processed_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=\"Processing files\"):\n",
    "        try:\n",
    "            # Read JSON file\n",
    "            with open(json_file, 'r', encoding='utf-8') as file:\n",
    "                json_data = json.load(file)\n",
    "            \n",
    "            # Convert to CSV\n",
    "            output_file = convert_json_to_csv(json_data, output_directory)\n",
    "            \n",
    "            if output_file:\n",
    "                processed_files.append((json_file, output_file))\n",
    "            else:\n",
    "                failed_files.append((json_file, \"No cricket_match_extended_ball data found\"))\n",
    "        except Exception as e:\n",
    "            failed_files.append((json_file, str(e)))\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {len(processed_files)} files\")\n",
    "    print(f\"Failed to process {len(failed_files)} files\")\n",
    "    \n",
    "    return processed_files, failed_files\n",
    "\n",
    "# Example usage - uncomment and update paths to use:\n",
    "# specific_input_dir = \"path/to/specific/json/folder\"\n",
    "# specific_output_dir = \"path/to/specific/output/folder\"\n",
    "# processed, failed = process_directory(specific_input_dir, specific_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process a Single File\n",
    "\n",
    "Use this cell if you want to process just one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(json_file_path, output_directory='.'):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        # Convert to CSV\n",
    "        output_file = convert_json_to_csv(json_data, output_directory)\n",
    "        \n",
    "        if output_file:\n",
    "            print(f\"Successfully processed {json_file_path} -> {output_file}\")\n",
    "            return output_file\n",
    "        else:\n",
    "            print(f\"No cricket_match_extended_ball data found in {json_file_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {json_file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage - uncomment and update path to use:\n",
    "# single_file = \"path/to/specific/json/file.json\"\n",
    "# result = process_single_file(single_file, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fcbb12c2c34fecbf3c7071c3770bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing JSON files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/20.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/16.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/6.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/7.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/17.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/21.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/10.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/1.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/11.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/2.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/12.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/13.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/3.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/8.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/22.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/18.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/4.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/14.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/15.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/5.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/19.json\n",
      "❌ Failed to convert: /Users/milangabriel/Downloads/gabscore/Scrapper/Match_Data_Extract/JSON/9.json\n"
     ]
    }
   ],
   "source": [
    "# Loop over JSON files in the input directory\n",
    "json_files = glob.glob(os.path.join(input_dir, \"*.json\"))\n",
    "\n",
    "for json_file in tqdm(json_files, desc=\"Processing JSON files\"):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        json_content = f.read()\n",
    "    \n",
    "    csv_path = convert_json_to_csv(json_content, output_dir=output_dir)\n",
    "    if csv_path:\n",
    "        print(f\"✅ Converted: {json_file} → {csv_path}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to convert: {json_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
